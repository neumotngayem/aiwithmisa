---
layout: post
title: "[Lý Thuyết] Bài 5 Hồi quy tuyến tính - Linear Regression"
subtitle: "Loạt bài học thuộc series AML"
date: 2020-12-12
author: "KyoHB"
header-img: "img/post-bg-aml.jpg"
tags: [AML]
---

Qua 3 bài đầu tiên về những khái niệm cơ bản trong ML, chúng ta đã biết có hai loại bài toán trong học giám sát (Supervised ML) đó là: regression và classification. Trong bài học ngày hôm 
nay chúng ta sẽ tìm hiểu supervised ML model đầu tiên - Linear Regression, đúng như theo tên gọi, nó được sử dụng để giải quyết các bài toán liên quan đến regression như dự đoán tuổi tác, chiều cao, giá nhà,...


Giả sử trong tập dữ liệu (dataset) của chúng ta có một biến độc lập (independent variable) là x và một biến phụ thuộc (dependent variable) là y. Từ đó chúng ta sẽ thiết lập mối liên hệ giữa chúng bằng một hàm số tuyến tính (linear function) như sau:

$y = w_{0} + w_{1}x$
{:.formula}

Trong hàm số trên chúng ta có thể để ý thấy có 2 trọng số (weight) đó là $w_{0}$ và $w_{1}$. Trong 2 trọng số này, $w_{0}$ chính là hệ số chặn (intercept), $w_{1}$ là hệ số nghiêng (slope) của đường tuyến tính (linear). 
Nếu các bạn còn nhớ trong bài 1 chúng ta có đề cập đến model parameter, trong Linear Regression, các trọng số $w_{0}$ và $w_{1}$ chính là các model parameter mà ML model cần tìm trong quá trình "học".

![]({{ site.baseurl }}/img/in-post/aml/linear-line.jpg)*Source: methods.sagepub.com*
{:.image-caption}

Linear Regression model có nhiệm vụ tìm ra $w_{0}$ và $w_{1}$ phù hợp (fit) với tập dữ liệu (dataset). Nhưng làm sao để Linear Regression model biết được $w_{0}$ và $w_{1}$ nào là phù hợp ? Chúng ta sẽ quay lại với khái niệm đã được nhắc đến trong bài 2, hàm mất mát (loss). Hàm mất mát được sử dụng trong LR model đó là Residual Sum of Square (RSS), và được tính theo công thức sau:

$RSS = \sum_{1}^{n} (y_{actual} - y_{predict})^{2}$
{:.formula}

Trong công thức trên n chính là số cá thể (instance) trong tập dữ liệu (dataset). Phần dư thừa (residual) giữa kết quả thực tế và dự đoán có thể nhìn thấy một cách trực quan hơn trong hình vẽ dưới đây:

![]({{ site.baseurl }}/img/in-post/aml/linear-regression-residual.png)*Source: sthda.com*
{:.image-caption}


Các gạch màu đỏ trong đồ thị trên chính các phần dư thừa (residual) được biểu thị bằng hình ảnh. Khi RSS nhỏ nhất, tổng các độ dài gạch đỏ này sẽ là ngắn nhất, và ta sẽ có một đường tuyến tính phù hợp nhất (best fit line) với các cá thể (instance) của tập dự liệu (dataset). Từ đường hồi quy tuyến tính (regression line) đã được xác định, chúng ta có thể dự đoán giá trị của biến phụ thuộc (dependent variable) y bằng cách ánh xạ (reflected) giá trị của biến không phụ thuộc (independent variable) x vào đường hồi quy tuyến tính (regression line).

Ở phần trên chúng ta chỉ sử dụng một biến không phụ thuộc (independent variable) x. Tuy nhiên các bài toán trong thực tế không chỉ dừng lại ở một biến không phụ thuộc (independent variable), ví dụ như giá nhà không chỉ phụ thuộc vào diện tích của ngôi nhà, nó còn phụ thuộc vào khoảng cách từ ngôi nhà đến trung tâm, số phòng ngủ, số nhà vệ sinh,... 
Cần nhắc lại một chút kiến thức chúng ta đã học trong bài 2, đó là các biến không phụ thuộc này cần đảm bảo không có mối tương quan với nhau lớn, nhằm tránh xảy ra hiện tượng cộng tuyến (collinearity) ảnh hướng đến độ chính xác của Linear Regression model. Với việc có thêm các biến không phụ thuộc (independent variable) chúng ta cần mở rộng hàm số tuyến tính (linear function) ra với nhiều các trọng số hơn, bài toán khi đó được gọi là Multiple Linear Regression. Hàm số tuyến tính (linear function) khi đó sẽ như sau:


$y = w_{0} + w_{1}x_{1} + w_{2}x_{2} +...+ w_{k}x_{k}$
{:.formula}

Trong đó k là số biến không phụ thuộc (independent variable).

Khi sự phân bố của các cá thể trong tập dữ liệu mang tính phi tuyến tính (nonlinear) chúng ta có thể sử dụng phương pháp biến đổi với các biến không phụ thuộc (independent variable) để chuyển LR sang Polynomial Regression nhằm giúp cải thiện hiệu năng (performance) của model. Hàm số đa thức (polynomial function) độ (degree) 2 có dạng như sau:

$y = w_{0} + w_{1}x\ + w_{2}x^{^{2}}$
{:.formula}

Việc tạo thêm một tính năng (feature) $x^{^{2}}$, vô hình chúng giúp tạo ra sự một phi tuyến tính (non linear) cho model. Dù vậy về bản chất $x^{^{2}}$ được tạo ra là từ $x$ nên các trọng số gắn với nó là $w_{1}$ và $w_{2}$ vẫn có mối quan hệ tuyến tính (linear relationship). Vì lý do đó Polynomial Regression được coi như một một trường hợp đặc biệt của Linear Regression.

![]({{ site.baseurl }}/img/in-post/aml/polynomial-regression.png)*Source: javatpoint.com*
{:.image-caption}

Chúng ta cũng cần lưu ý kiểm tra hiệu năng (performance) của Polynomial Regression trong tập xác nhận (validation set) và tập kiểm tra (testing set) vì việc "uốn cong" theo các cá thể (instance) ở tập huấn luyện (training set) có thể gây ra vấn đề quá vừa vặn (overfit).
