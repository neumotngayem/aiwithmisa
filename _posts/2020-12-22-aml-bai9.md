---
layout: post
title: "[Lý Thuyết] Bài 9 Phân loại Naïve Bayes - Naïve Bayes classifier"
subtitle: "Loạt bài học thuộc series AML"
date: 2020-12-22
author: "KyoHB"
header-img: "img/post-bg-aml.jpg"
tags: [AML]
---

Trong bài học này chúng ta sẽ tìm hiểu về một supervised ML model phi tuyến tính (non-linear) sử dụng trong các bài toán phân loại (classication) được gọi là phân loại Naïve Bayes.


## Phân phối chuẩn - Normal distribution

Đầu tiên chúng ta cùng ôn lại về phân phối chuẩn (Normal distribution / Gaussian distribution). Phân phối chuẩn sẽ cho ra một biểu đồ mật độ xác suất (probability density plot) như sau:

![]({{ site.baseurl }}/img/in-post/aml/normal_distri.png)


Những đặc trưng của biểu đồ này đó là nó có hình chuông (bell curve), đối xứng qua giá trị trung bình (mean) $\mu$. Chúng ta có hàm số mật độ xác xuất (probability density function) để tính mật độ xác xuất (probability density) của một biến $x$ là $P(x)$ trong một phân phối chuẩn (normal distribution) như sau:

$P(x) = \frac{e^{-(x - \mu)^{2}/(2\sigma^{2}) }} {\sigma\sqrt{2\pi}}$
{:.formula}

Trong đó $\mu$ là gì trị trung bình (mean), $\sigma^{2}$ là phương sai (variance)

## Định lý Bayes - Bayes Theorem

Xét trong một bài toán ở trường trong đó chúng ta muốn vẽ biểu đồ mật độ xác xuất (probability density plot) về chiều cao của nhóm học sinh nam và nữ, chúng ta có thể thu thập và tính mật độ xác xuất (probability density) của một chiều cao $x$  trực quan theo công thức sau:

$P(x) = \frac{Số\\:lượng\\:học\\:sinh\\:cùng\\:nhóm,\\:cùng\\:chiều\\:cao}{Tổng\\:số\\:học\\:sinh\\:thuộc\\:nhóm\\:đó}$
{:.formula}

Biều đồ mật độ xác xuất (probability density plot) của chúng ta khi ấy sẽ như sau:

![]({{ site.baseurl }}/img/in-post/aml/prob_1.png)

Dựa vào biểu đồ xác xuất trên chúng ta có thể biết khả năng (likelihood) của chiều cao một học sinh cho biết nhóm của học sinh đó (Nam hoặc Nữ) được ký hiệu là $P(x\|c)$ trong đó $x$ là chiều cao, $c$ là nhóm học sinh.


![]({{ site.baseurl }}/img/in-post/aml/prob_2.png)


Trong biều đồ mật độ xác xuất (probability density plot) trên vùng mà cả của 2 nhóm gối (overlap) lên nhau chúng ta có $P(155 | Nữ) = 0.35$  và $P(155 | Nam) = 0.2$ như vậy nếu chúng ta có một học sinh cao 1m55 thì học sinh này có thể là nam hoặc nữ nhưng có khả năng (likelihood) học sinh đó là nữ cao hơn.
Nếu xét sang những vùng không gối lên nhau (overlap) như chiều cao của học sinh đó là 1m7 thì khả năng (likelihood) học sinh đó là nam sẽ nhiều hơn.

Từ đây chúng ta có thể mường tượng rằng nếu chúng ta có thể tính được xác suất của một cá thể (instance) thuộc nhóm $c$ cho biết biến không phụ thuộc $x$ - $P(c\|x)$. Dựa vào xác suất này chúng ta sẽ phân loại được đối tượng. Định lý Bayesian (Bayesian Theorem) cho chúng ta công thức (formula) sau:

$P(c\|x)=\frac{P(x\|c)P(c)}{P(x)}$
{:.formula}

Trong đó $P(c\|x)$ được gọi là xác suất hậu nghiệm (posterior probability), $P(x\|c)$ được gọi là khả năng (likelihood), $P(c)$ được gọi là xác suất nhóm tiên nghiệm (class prior probability) và $P(x)$ là xác suất dự đoán tiên nghiệm (predictor prior probability)

Trong đó $P(x\|c)$ - khả năng (likelihood) đã được nhắc đến ở trên, còn hai xác suất tiên nghiệm kia là gì ?

1. Xác suất nhóm tiên nghiệm (class prior probability) $P(c)$: cho biết xác suất của nhóm đó trong quần thể (population). Ví dụ như trường học trên có 40% là nữ và 60% là nam chúng ta sẽ có $P(Nam)$ = 0.6 và $P(Nữ)$ = 0.4
2. Xác suất dự đoán tiên nghiệm (predictor prior probability) $P(x)$: cho biết xác suất của cá thể đó trong quần thể (population). Ví dụ $P(155)$ là số học sinh cao 1m55 trên toàn bộ số học sinh của trường (không phân biệt nhóm).

Quay trở về với bài toán ban đầu, chúng ta sẽ tính $P(Nam\|155)$ và $P(Nữ\|155)$ dựa trên định lý Bayes (Bayes Theorem) nhé:

$P(Nam\|155)=\frac{P(155\|Nam)P(Nam)}{P(155)} =\frac{0.2\times 0.6}{P(155)}=\frac{0.12}{P(155)}$
{:.formula}

$P(Nữ\|155)=\frac{P(155\|Nữ)P(Nữ)}{P(155)}=\frac{0.35\times 0.4}{P(155)}=\frac{0.14}{P(155)}$
{:.formula}

Vì chúng ta chỉ quan tâm đến độ lớn của $P(Nam\|155)$ và $P(Nữ\|155)$ xem bên nào có xác suất lớn hơn, nên chúng ta có thể loại bỏ phần mẫu số $P(155)$ ở đây và cũng như ở những bài toán tiếp theo. Có thể thấy $P(Nữ\|155)$ lớn hơn $P(Nam\|155)$ một chút xíu nên nếu một học sinh có chiều cao là 1m55 chúng ta có thể phân loại học sinh này là nữ.

## Giả định Naïve - Naïve Assumption

Trong trường hợp chúng ta có nhiều hơn một biến không phụ thuộc (independent variable) x, như không chỉ chiều cao của học sinh chúng ta có cả cỡ giày, sải tay,... Chúng ta sẽ tính $P(x\|c)$ trong trường hợp đó như nào ? Khi đó chúng ta sẽ áp dụng giả định (assumption) Naïve, trong đó các biến không phụ thuộc (independent variable) này là độc lập với nhau, từ đó:

$P(x\|c) = P(x_{1}\|c)\times P(x_{2}\|c)\times P(x_{3}\|c)\times...\times P(x_{n}\|c)$
{:.formula}

Trong đó n là số biến không phụ thuộc (independent variable)

Kết hợp với định lý Bayes ở trên, chúng ta sẽ có công thức (formula) như sau:

$P(c\|x_{1},x_{2},x_{3},...x_{n})\propto P(x_{1}\|c)P(x_{2}\|c)P(x_{3}\|c)...P(x_{n}\|c)P(c)$
{:.formula}

Trong đó $\propto$ là tỉ lệ (proportion) với 

Đây là công thức được sử dụng trong phân loại Naïve Bayes (Naïve Bayes classifier). 

Một cách tổng quát hóa trong phân loại Naïve Bayes (Naïve Bayes classifier) chúng ta sẽ tính các tích ở phần tử số, và tìm xem nhóm (class) $c$ nào có phần tử số lớn nhất (vì phần mẫu số luôn bằng nhau giữa các nhóm). Chúng được viết lại theo công thức toán học như sau:

$\hat{c} = \underset{c}{arg max}P(c)\prod_{i=1}^{n}P(x_{i}\|c)$
{:.formula}

Trong đó $\hat{c}$ là nhóm dự đoán, $\underset{c}{arg max}$ là tìm đối số (argument) nhóm (class) c cho kết quả hàm số (function) lớn nhất.

## Ví dụ

Chúng ta có một bảng thống kê các thông số thời tiết bao gồm quang cảnh (outlook), nhiệt độ (temparature), độ ẩm (humidity), gió (windy) và biến phụ thuộc (dependent variable) ở đây là có hoặc không chơi tennis.

![]({{ site.baseurl }}/img/in-post/aml/example_naive.png)

Xét trong điều kiện như sau: quang cảnh là nắng (sunny), nhiệt độ là 66, độ ẩm là 90, có gió thì có chơi tennis không ? Chúng ta sẽ áp dụng phân loại Naïve Bayes để quyết định nhé !

Đầu tiên chúng ta sẽ tính xác suất có chơi tennis trong điều kiện trên:

$P(out = sunny\|Yes) = \frac{2}{9}$
{:.formula}

$P(wind = true\|Yes) = \frac{3}{9}$
{:.formula}

Đối với các biến dạng số (numerical variable) chúng ta sẽ sử dụng phân phối chuẩn (normal distribution) để tính khả năng (likelihood) như sau: 

$P(temp = 66\|Yes) = \frac{e^{-(66 - \mu)^{2}/(2\sigma^{2}) }} {\sigma\sqrt{2\pi}} = \frac{e^{-(66 - 73)^{2}/(2\times 6.2^{2}) }} {6.2\sqrt{2\pi}} = 0.034$
{:.formula}
$P(hum = 90\|Yes) = \frac{e^{-(90 - \mu)^{2}/(2\sigma^{2}) }} {\sigma\sqrt{2\pi}} = \frac{e^{-(90 - 79.1)^{2}/(2\times 10.2^{2}) }} {10.2\sqrt{2\pi}} = 0.0221$
{:.formula}


$P(Yes \| out=sunny, temp=66, hum=90, wind=true)$
$\propto P(out = sunny\|Yes) \times P(temp = 66\|Yes) \times P(hum = 90\|Yes) \times P(wind = true\|Yes) \times P(Yes)$
$\propto  \frac{2}{9} \times 0.034 \times 0.0221 \times \frac{3}{9} \times \frac{9}{14}$

$\propto 0.000036$

Tương tự với xác suất không chơi tennis trong điều kiện trên:

$P(No \| out=sunny, temp=66, hum=90, wind=true)$
$\propto P(out = sunny\|No) \times P(temp = 66\|No) \times P(hum = 90\|No) \times P(wind = true\|No) \times P(No)$
$\propto  \frac{3}{5} \times 0.0291 \times 0.038 \times \frac{3}{5} \times \frac{5}{14}$

$\propto 0.000136$

Do $P(Yes \| out=sunny, temp=66, hum=90, wind=true) < P(No \| out=sunny, temp=66, hum=90, wind=true)$ nên trong điều kiện trên sẽ không chơi tennis.

## Laplace Smoothing

Nếu để ý ở bảng số liệu trên chúng ta nhận thấy $P(out = overcast\|No) = 0$, khi đó chúng ta thực hiện tính xác suất không chơi tennis sẽ bằng 0 dù cho nhiệt độ, độ ẩm, gió là thế nào. Để tránh trường hợp này xảy ra khả năng (likelihood) của chúng ta sẽ được tính như sau:

$P(out=overcast\|Yes) = \frac{Số\\:lần\\:outlook\\:là\\:overcast\\:và\\:không\\:chơi\\:tennis + \alpha }{Tổng\\:số\\:lần\\:không\\:chơi\\:tennis + \alpha\times K}$

Trong đó $\alpha$ là bậc (degree) của smoothing, K là số nhóm (class) của biến phụ thuộc (dependent variable)

Trong trường hợp $\alpha = 1$, chúng ta có thể gọi là add-one smoothing. Thử áp dụng các con số để tính $(out = overcast\|No)$ sau khi smoothing nhé:

$P(out=overcast\|Yes) = \frac{0 + 1}{5 + 1\times2} = 0.14$

Một lưu ý nhỏ là chúng ta cũng sẽ cần áp dụng Laplace Smoothing với các khả năng (likelihood) khác khi tính xác suất chơi và không chơi tennis.

## Kernel Density

Trong bài toán có sử dụng biến dạng số (numerical variable) như trên, chúng ta đang giả định rằng chúng được phân phối chuẩn (normal distribution). Tuy nhiên giả định này có thể sai so với phân phối thực tế, trong trường hợp đó để cải thiện hiệu năng chúng ta có thể sử dụng phương pháp ước lượng Kernel Density (Kernel Density Estimation - KDE). Khi đó mật độ xác suất (probability density) sẽ được estimate theo KDE và sử dụng khi tính toán, giúp tăng độ chính xác của model.

## Kết

Như vậy trong bài học ngày hôm nay chúng ta đã làm quen với phân loại Naïve Bayes, một phân loại khá đặc biệt vì việc huấn luyện (training) được dựa trên việc tính toán những giá trị của các cá thể (instance). Do đặc tính này mà một Naïve Bayes model có thể được huấn luyện ngay với cả một tập dữ liệu (dataset) nhỏ. Ở bài học sau chúng ta sẽ thực hành triển khai một Naïve Bayes model bằng Python, hãy đón đọc nhé 💪 .