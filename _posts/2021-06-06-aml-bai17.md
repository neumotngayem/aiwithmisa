---
layout: post
title: "[Lý Thuyết] Bài 16 Phân cụm k-Means - k-Means Clustering"
subtitle: "Loạt bài học thuộc series AML"
date: 2021-06-06
author: "KyoHB"
header-img: "img/post-bg-aml.jpg"
tags: [AML]
---

Qua 15 bài học và tìm hiểu về các ML model khác nhau, chúng ta đã rất quen thuộc với các ML model học giám sát (supervised learning)
, trong đó quá trình huấn luyện của chúng ta đòi hỏi dữ liệu phải được gán nhãn (label) trước. Hôm nay chúng ta sẽ tìm hiểu một phương pháp huấn luyện khác - phương pháp
học không giám sát (unsupervised learning), cùng tìm hiểu xem với phương pháp này quá trình huấn luyện (training) của chúng ta có
những gì khác biệt nhé !

## Học không giám sát, phân cụm

Trong học không giám sát (unsupervised learning), tập dữ liệu (dataset) của chúng ta không được gán nhãn (label) cụ thể như đây là chó, đây là mèo,... 
Phân cụm là một hình thức học không giám sát (unsupervised learning), trong đó chúng ta cần ML model đánh giá và gom các cá thể có chung kiểu mẫu (pattern) về một cụm. 
Để làm được điều này chúng ta sẽ chọn ra một tiêu chuẩn (criteria) và thực hiện phân cụm theo tiêu chuẩn này. Kết quả cuối cùng thu được của chúng ta sẽ là những 
cụm cá thế, cá thể ở các cụm khác nhau sẽ khác biệt với nhau, các cá thể (instance) trong cùng một cụm sẽ tương đồng với nhau trên tiêu chuẩn đã được đặt ra.

![]({{ site.baseurl }}/img/in-post/aml/clustering.png)


Hình trên là một ví dụ về việc phân cụm không giám sát (unsupervised). Tùy thuộc vào tiêu chuẩn phân cụm 
mà chúng ta lựa chọn là: hình khối, kích cỡ hay màu sắc chúng ta sẽ thu được các kết quả phân cụm khác nhau.

## Các loại phân cụm

Trong phân cụm chúng ta có 2 loại phân cụm chính đó là phân cụm cứng (hard clustering) và phân cụm mềm 
(soft clustering). Trong phân cụm cứng (hard clustering), mỗi cá thể (instance) sẽ chỉ thuộc về một cụm duy nhất hoặc 
không thuộc cụm nào. Ngược lại, trong phân cụm mềm (soft clustering) các cá thể (instance) có thể thuộc về nhiều hơn một cụm, kết quả
phân cụm mềm (soft clustering) cho chúng ta biết xác suất (probability) của các cá thể (instance) thuộc về từng cụm.

![]({{ site.baseurl }}/img/in-post/aml/soft_hard_clustering.png)*Source: medium.com*
{:.image-caption}

Để phân cụm cứng (hard clustering) chúng ta có 2 phương pháp đó là phân cụm theo thứ bậc (hierarchical) và phân cụm theo phần (partitional). 

![]({{ site.baseurl }}/img/in-post/aml/taxonomy_clustering.png)

Đúng như tên gọi phân cụm theo thứ bậc (hierarchical) sẽ cố gắng phân cấp các cụm với 2 chiến lược đó là phân tách (divisive) và gom tụ (agglomerative).
Trong cách tiếp cận phân tách (divisive), chúng ta sẽ coi toàn bộ các cá thể (instance) đều thuộc một cụm sau đó tiến hành phân tách dần
cho đến khi mỗi cá thể (instance) là một phân cụm riêng lẻ. Ở chiều ngược lại, cách tiếp cận gom tụ (agglomerative), ban đầu mỗi cá thể (instance) sẽ là một nhóm (cluster) riêng lẻ,
chúng ta sẽ tiến hành gộp các cụm riêng lẻ này lại, cho đến khi tất cả đều thuộc về một cụm.

![]({{ site.baseurl }}/img/in-post/aml/algo_divi.png)*Source: researchgate.net*
{:.image-caption}

Kết quả của quá trình phân cụm theo thứ bậc (hierarchical) được minh họa bằng cây phả hệ - dendrogram, biểu thị phân cấp giữa các cụm. Đây cũng chính là cơ sở để chúng ta quyết định
số cụm cuối cùng mà quần thể có phân cụm.

![]({{ site.baseurl }}/img/in-post/aml/dendogram.png)*Source: rpubs.net*
{:.image-caption}

Đối với phân cụm theo phần (partitional), chúng ta phải lựa chọn trước số cụm trước khi tiến hành phân cụm. Thêm vào đó mỗi lần phân cụm theo phần (partitional) chúng ta sẽ
thu được các kết quả phân cụm khác nhau, trái ngược với tính xác định (deterministic) khi phân cụm theo thứ bậc (hierarchical). Phân cụm k-Means là một trong những thuật toán
phân cụm theo phần (partitional) rất nổi tiếng.

## Phân cụm k-Means
Trong phân cụm k-Means, chúng ta sẽ có các bước cơ bản sau:
1. Chọn ngẫu nhiên k trọng tâm (centroid) của các cụm.
2. Dựa trên khoảng cách với các trọng tâm (centroid) của các cụm để phân phối và sắp xếp các cá thể (instance) vào các cụm.
3. Tính trọng tâm (centroid) mới của các cụm sau khi được sắp xếp.
4. Lặp lại bước số 2 cho đến khi trọng tâm (centroid) của các cụm không thay đổi nữa.

![]({{ site.baseurl }}/img/in-post/aml/kmeans.png)*Source: wikipedia.org*
{:.image-caption}

Trong mục đầu tiên chúng ta có đề cập đến tiêu chuẩn để phân cụm và ở đây tiêu chuẩn đó chính là khoảng cách cách so với trọng tâm 
(centroid). Để tính khoảng cách này chúng ta có thể sử dụng các khoảng cách Euclidean, Weighted Euclidean, Minkowski, Manhattan,... Phổ biết nhất vẫn là Euclidean

## Ví dụ
Chúng ta sẽ cùng làm một ví dụ nhỏ sau đây để hiểu hơn về cách k-Means hoạt động nhé !
Giả sử chúng ta có một tập dữ liệu (dataset) với một 1 biến phụ thuộc (dependent variable) X và 1 biết không phụ thuộc (independent variable) Y có giá trị như sau:


| X  | Y |
|----|---|
| 5  | 6 |
| 1  | 2 |
| 10 | 4 |
| 8  | 3 |
| 12 | 1 |
| 1  | 5 |

Quá trình tính toán theo thuật toán k-Means với k bằng 2 sẽ diễn ra như sau:

![]({{ site.baseurl }}/img/in-post/aml/kmeans_cal.png)

Sau lần tính toán thứ 2, các cá thể (instance) của cả 2 cụm đều không thay đổi nên trọng tâm của các cụm cũng sẽ không thay đổi. Do đó chúng ta dừng tính toán tiếp. Có thể thấy
sau quá trình phân cụm kết thúc, sự chênh lệch lớn trong khoảng cách của các cá thể (instance) với các trọng tâm (centroid) cho thấy tính phân biệt cao của từng cụm.

## Kết
Như vậy trong bài học ngày hôm nay chúng ta đã tìm hiểu một phương pháp huấn luyện ML mới đó là phương pháp học không giám sát (unsupervised learning). Phương pháp này
được áp dụng trong các bài toán không xác định được nhãn (label) cụ thể của các cá thể (instance) ví dụ như phân loại nhóm khách hàng, phân loại bất thường (anomaly detection),...
Phân cụm k-Means cũng sẽ là bài lý thuyết cuối cùng trong chuỗi các bài học về ứng dụng ML, các bạn hãy chờ đón các phần tiếp theo nhé !

